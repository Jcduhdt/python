{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#基于urllib，采用Apache2 Licensed开源协议的HTTP库\n",
    "import requests\n",
    "#美丽汤，网页解析库，可代替正则表达式\n",
    "from bs4 import BeautifulSoup\n",
    "#sys模块包含了与python解释器和它环境有关的函数\n",
    "import sys\n",
    "#正则表达式，检查一个字符串是否与某种模式匹配\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please input the url of novel website\n",
      "> http://www.bxwx4.com/\n",
      "Please input the url of book's list\n",
      "> http://www.bxwx4.com/146521_43/\n",
      "悲伤逆流成河下载完成\n"
     ]
    }
   ],
   "source": [
    "#创建一个类，就像requests.get()一样，父子节点，包含关系\n",
    "class downloader(object):\n",
    "    #初始化\n",
    "    def __init__(self):\n",
    "        #获取小说网址\n",
    "        #比如http://www.bxwx4.com/\n",
    "        print(\"Please input the url of novel website\")\n",
    "        self.server = input('> ')\n",
    "        #获取需要下载的小说url\n",
    "        #比如http://www.bxwx4.com/104039_41/\n",
    "        print(\"Please input the url of book's list\")\n",
    "        self.target = input('> ')\n",
    "        #模拟浏览器登录，我从电脑上的世界之窗复制的\n",
    "        self.headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/64.0.3282.140 Safari/537.36 Edge/17.17134'}\n",
    "        #存放章节名\n",
    "        self.names = []\n",
    "        #存放章节链接\n",
    "        self.urls = []\n",
    "        #章节数\n",
    "        self.nums = 0\n",
    "        \n",
    "    #获取小说名字，在创建txt文档时使用\n",
    "    def get_bookname(self):\n",
    "        #发送get请求，得到url源码\n",
    "        req = requests.get(url = self.target,headers = self.headers)\n",
    "        #.text转化成文本？\n",
    "        html = req.text\n",
    "        #这里使用的正则，因为现在的我无法用美丽汤来获取。。\n",
    "        bookname = re.findall(r'<h1>(.*?)</h1>',html)[0]\n",
    "        return bookname\n",
    "    \n",
    "    #得到需要下载的链接\n",
    "    def get_download_url(self):\n",
    "        req = requests.get(url = self.target,headers = self.headers)\n",
    "        html = req.text\n",
    "        #用美丽汤来解析，要添加html.parser来解析，不然会报错，至少在我这是\n",
    "        soup = BeautifulSoup(html,'html.parser')\n",
    "        #根据html源码查看自己所需要的部分，这里就是find章节目录\n",
    "        div = soup.find_all('div',id='list')\n",
    "        #这个[0],现在也不太清楚，似乎是数组的第一个，也不太清楚html中的存放\n",
    "        lists = BeautifulSoup(str(div[0]),'html.parser')\n",
    "        #似乎每个网站存放list都是<a href=\"\".../a>\n",
    "        #找出所有章节\n",
    "        lists_a = lists.find_all('a')\n",
    "        self.nums = len(lists_a)\n",
    "        for each in lists_a:\n",
    "            #添加章节名进数组\n",
    "            self.names.append(each.string)\n",
    "            #添加每一章节对应的url\n",
    "            self.urls.append(self.server + each.get('href'))\n",
    "            \n",
    "    #获取每一章的内容        \n",
    "    def get_contents(self,target):\n",
    "        req = requests.get(url = target)\n",
    "        html = req.text\n",
    "        soup = BeautifulSoup(html,'html.parser')\n",
    "        texts = soup.find_all('div',id = 'content')\n",
    "        #源码中会有很多<br />或者其他符号，用\\n替换\n",
    "        #有些网站会隐藏正文内容，爬取不到试了网友说的使用\n",
    "        #texts[0].text.replace('','\\n').replace('br /','\\n')\n",
    "        #正文是显示出来了不过是一个字一行，也不知道怎么解决\n",
    "        texts = texts[0].text.replace('br /','\\n')\n",
    "        return texts\n",
    "    \n",
    "    #将内容写入文件中\n",
    "    def writer(self,name,path,text):\n",
    "        write_flag = True\n",
    "        #打开路径，没有就创建，以utf-8的编码方式\n",
    "        with open(path,'a',encoding='utf-8') as f:\n",
    "            f.write(name +'\\n')\n",
    "            #对章节内容按一行一行输入\n",
    "            f.writelines(text)\n",
    "            f.write('\\n\\n')\n",
    "            \n",
    "            \n",
    "#主函数\n",
    "if __name__ == \"__main__\":\n",
    "    dl = downloader()\n",
    "    #父子，包含关系\n",
    "    #因为初始化的时候给了目标网址，self已经在downloader这个类中共享了\n",
    "    #这个函数使用后已经获取到完整的章节名数组，以及对应的url数组\n",
    "    dl.get_download_url()\n",
    "    bookname = dl.get_bookname()\n",
    "    for i in range(dl.nums):\n",
    "        #self已经在类中用了，只需输入后面三个参数\n",
    "        #一章节为单位进行写入\n",
    "        #循环一次写完一章节\n",
    "        #以%后的bookname传入%s\n",
    "        dl.writer(dl.names[i],'%s.txt' % bookname,dl.get_contents(dl.urls[i]))\n",
    "        #显示下载进度，以百分比形式，精度为小数点后三位\n",
    "        #float(i*100/dl.nums)传入%.3f,  %%输出为%\n",
    "        # \\r 默认表示将输出的内容返回到第一个指针，这样的话，后面的内容会覆盖前面的内容\n",
    "        sys.stdout.write(\"已下载%.3f%%\" % float(i*100/dl.nums) + '\\r')\n",
    "        #stdout有缓冲区，该函数用于刷新stdout\n",
    "        sys.stdout.flush()\n",
    "    print('%s下载完成' % bookname)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#一些问题\n",
    "#一：用jupyter运行时进度不能显示，把code保存成.py文件用powershell可以显示\n",
    "#二：很多时候，在下载中途会与服务器断开连接导致下载失败，只能下载部分。有时候过段时间就行，就像重启大法一样\n",
    "#三：该code过于简单，就像是在理想情况下做仿真一样，没有触及放爬虫机制\n",
    "#四：下载速度过慢，没有使用多线程\n",
    "#五：代理ip\n",
    "#六：还可以尝试一下从第一章开始下载，根据下一章链接来下载全文，不再章节目录获取所有url\n",
    "#七：为了代码美观，就下载了悲伤逆流成河这篇不大的小说，因为测试了基本玄幻小说都死在了三次无法链接上，就有错误报出来\n",
    "#最后，这个代码是根据https://blog.csdn.net/c406495762/article/details/78123502来编写的"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
