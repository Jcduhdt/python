{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 官方文档url：http://beautifulsoup.readthedocs.io/zh_CN/latest/\n",
    "from bs4 import BeautifulSoup\n",
    "from bs4 import element\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<html>\n",
      " <head>\n",
      "  <title>\n",
      "   Jack_Cui\n",
      "  </title>\n",
      " </head>\n",
      " <body>\n",
      "  <p class=\"title\" name=\"blog\">\n",
      "   <b>\n",
      "    My Blog\n",
      "   </b>\n",
      "  </p>\n",
      "  <li>\n",
      "   <!--注释-->\n",
      "  </li>\n",
      "  <a class=\"sister\" href=\"http://blog.csdn.net/c406495762/article/details/58716886\" id=\"link1\">\n",
      "   Python3网络爬虫(一)：利用urllib进行简单的网页抓取\n",
      "  </a>\n",
      "  <br/>\n",
      "  <a class=\"sister\" href=\"http://blog.csdn.net/c406495762/article/details/59095864\" id=\"link2\">\n",
      "   Python3网络爬虫(二)：利用urllib.urlopen发送数据\n",
      "  </a>\n",
      "  <br/>\n",
      "  <a class=\"sister\" href=\"http://blog.csdn.net/c406495762/article/details/59488464\" id=\"link3\">\n",
      "   Python3网络爬虫(三)：urllib.error异常\n",
      "  </a>\n",
      "  <br/>\n",
      " </body>\n",
      "</html>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#自己定义了一个html\n",
    "html = \"\"\"\n",
    "<html>\n",
    "<head>\n",
    "<title>Jack_Cui</title>\n",
    "</head>\n",
    "<body>\n",
    "<p class=\"title\" name=\"blog\"><b>My Blog</b></p>\n",
    "<li><!--注释--></li>\n",
    "<a href=\"http://blog.csdn.net/c406495762/article/details/58716886\" class=\"sister\" id=\"link1\">Python3网络爬虫(一)：利用urllib进行简单的网页抓取</a><br/>\n",
    "<a href=\"http://blog.csdn.net/c406495762/article/details/59095864\" class=\"sister\" id=\"link2\">Python3网络爬虫(二)：利用urllib.urlopen发送数据</a><br/>\n",
    "<a href=\"http://blog.csdn.net/c406495762/article/details/59488464\" class=\"sister\" id=\"link3\">Python3网络爬虫(三)：urllib.error异常</a><br/>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "soup = BeautifulSoup(html,'html.parser')\n",
    "#格式化输出，把代码格式搞标准一点\n",
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<title>Jack_Cui</title>\n",
      "************************\n",
      "<head>\n",
      "<title>Jack_Cui</title>\n",
      "</head>\n",
      "************************\n",
      "<a class=\"sister\" href=\"http://blog.csdn.net/c406495762/article/details/58716886\" id=\"link1\">Python3网络爬虫(一)：利用urllib进行简单的网页抓取</a>\n",
      "************************\n",
      "<p class=\"title\" name=\"blog\"><b>My Blog</b></p>\n"
     ]
    }
   ],
   "source": [
    "#BS方便的获取Tags(HTML中的一个个标签)\n",
    "#查找的是所有内容中的第一个符合要求的标签\n",
    "print(soup.title)\n",
    "print('************************')\n",
    "print(soup.head)\n",
    "print('************************')\n",
    "print(soup.a)\n",
    "print('************************')\n",
    "print(soup.p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'bs4.element.Tag'>\n"
     ]
    }
   ],
   "source": [
    "#验证这些对象的类型\n",
    "print(type(soup.title))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[document]\n",
      "title\n",
      "***********************************\n",
      "{'class': ['sister'], 'href': 'http://blog.csdn.net/c406495762/article/details/58716886', 'id': 'link1'}\n",
      "***********************************\n",
      "['sister']\n",
      "['sister']\n"
     ]
    }
   ],
   "source": [
    "#对于tag 两个重要属性name和attrs\n",
    "#soup对象特殊，其name为[document]，其他内部标签即标签本身名称\n",
    "print(soup.name)\n",
    "print(soup.title.name)\n",
    "print('***********************************')\n",
    "#得到类型为一个字典\n",
    "print(soup.a.attrs)\n",
    "print('***********************************')\n",
    "#获取单独的某个属性\n",
    "print(soup.a['class'])\n",
    "print(soup.a.get('class'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jack_Cui\n"
     ]
    }
   ],
   "source": [
    "#NavigableString\n",
    "#得到了标签内容，要获取标签内部文字用.string\n",
    "print(soup.title.string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "********************\n",
      "[document]\n",
      "********************\n",
      "{}\n"
     ]
    }
   ],
   "source": [
    "#BeautifulSoup\n",
    "#BeautifulSoup 对象表示的是一个文档的全部内容.大部分时候,可以把它当作 Tag 对象，\n",
    "#是一个特殊的 Tag，我们可以分别获取它的类型，名称，以及属性：\n",
    "print(type(soup.name))\n",
    "print('**'*10)\n",
    "print(soup.name)\n",
    "print('**'*10)\n",
    "print(soup.attrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<li><!--注释--></li>\n",
      "********************\n",
      "注释\n",
      "********************\n",
      "<class 'bs4.element.Comment'>\n",
      "********************\n",
      "注释\n"
     ]
    }
   ],
   "source": [
    "#Comment\n",
    "#Comment对象是一个特殊类型的NavigableString对象，其实输出的内容仍然不包括注释符号\n",
    "#需要好好对待这个问题\n",
    "print(soup.li)\n",
    "print('**'*10)\n",
    "print(soup.li.string)\n",
    "print('**'*10)\n",
    "print(type(soup.li.string))\n",
    "print('**'*10)\n",
    "#先判断其类型，发现是个Comment类型，所以在使用前做一个判断\n",
    "if type(soup.li.string) == element.Comment:\n",
    "    print(soup.li.string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', <p class=\"title\" name=\"blog\"><b>My Blog</b></p>, '\\n', <li><!--注释--></li>, '\\n', <a class=\"sister\" href=\"http://blog.csdn.net/c406495762/article/details/58716886\" id=\"link1\">Python3网络爬虫(一)：利用urllib进行简单的网页抓取</a>, <br/>, '\\n', <a class=\"sister\" href=\"http://blog.csdn.net/c406495762/article/details/59095864\" id=\"link2\">Python3网络爬虫(二)：利用urllib.urlopen发送数据</a>, <br/>, '\\n', <a class=\"sister\" href=\"http://blog.csdn.net/c406495762/article/details/59488464\" id=\"link3\">Python3网络爬虫(三)：urllib.error异常</a>, <br/>, '\\n']\n"
     ]
    }
   ],
   "source": [
    "#遍历文档，直接子节点(不含孙节点)\n",
    "#tag的content属性可以将tag的子节点以列表的方式输出\n",
    "print(soup.body.contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<p class=\"title\" name=\"blog\"><b>My Blog</b></p>\n"
     ]
    }
   ],
   "source": [
    "#列表索引获取元素\n",
    "print(soup.body.contents[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "<p class=\"title\" name=\"blog\"><b>My Blog</b></p>\n",
      "\n",
      "\n",
      "<li><!--注释--></li>\n",
      "\n",
      "\n",
      "<a class=\"sister\" href=\"http://blog.csdn.net/c406495762/article/details/58716886\" id=\"link1\">Python3网络爬虫(一)：利用urllib进行简单的网页抓取</a>\n",
      "<br/>\n",
      "\n",
      "\n",
      "<a class=\"sister\" href=\"http://blog.csdn.net/c406495762/article/details/59095864\" id=\"link2\">Python3网络爬虫(二)：利用urllib.urlopen发送数据</a>\n",
      "<br/>\n",
      "\n",
      "\n",
      "<a class=\"sister\" href=\"http://blog.csdn.net/c406495762/article/details/59488464\" id=\"link3\">Python3网络爬虫(三)：urllib.error异常</a>\n",
      "<br/>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#children 返回的不是list 不过可以通过遍历获取所有子节点，是一个list生成器对象\n",
    "for child in soup.body.children:\n",
    "    print(child)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<a class=\"sister\" href=\"http://blog.csdn.net/c406495762/article/details/58716886\" id=\"link1\">Python3网络爬虫(一)：利用urllib进行简单的网页抓取</a>, <a class=\"sister\" href=\"http://blog.csdn.net/c406495762/article/details/59095864\" id=\"link2\">Python3网络爬虫(二)：利用urllib.urlopen发送数据</a>, <a class=\"sister\" href=\"http://blog.csdn.net/c406495762/article/details/59488464\" id=\"link3\">Python3网络爬虫(三)：urllib.error异常</a>]\n"
     ]
    }
   ],
   "source": [
    "#搜索文档树\n",
    "#fian_all(name, attrs, recursive, text, limit, **kwargs)\n",
    "#name 参数可以查找所有名字为 name 的tag,字符串对象会被自动忽略掉\n",
    "#最简单的过滤器是字符串，比如查找所有<a>\n",
    "print(soup.find_all('a'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "body\n",
      "b\n",
      "br\n",
      "br\n",
      "br\n"
     ]
    }
   ],
   "source": [
    "#传递正则表达式\n",
    "#如果传入正则表达式作为参数,Beautiful Soup会通过正则表达式的 match() 来匹配内容.\n",
    "#下面例子中找出所有以b开头的标签,这表示<body>和<b>标签都应该被找到\n",
    "for tag in soup.find_all(re.compile('^b')):\n",
    "    print(tag.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<title>Jack_Cui</title>, <b>My Blog</b>]\n"
     ]
    }
   ],
   "source": [
    "#传递列表\n",
    "#如果传入列表参数，Beautiful Soup会将与列表中任一元素匹配的内容返回，\n",
    "#下面代码找到文档中所有<title>标签和<b>标签：\n",
    "print(soup.find_all(['title','b']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "html\n",
      "head\n",
      "title\n",
      "body\n",
      "p\n",
      "b\n",
      "li\n",
      "a\n",
      "br\n",
      "a\n",
      "br\n",
      "a\n",
      "br\n"
     ]
    }
   ],
   "source": [
    "#传递True\n",
    "# True 可以匹配任何值,下面代码查找到所有的tag,但是不会返回字符串节点\n",
    "for tag in soup.find_all(True):\n",
    "    print(tag.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<p class=\"title\" name=\"blog\"><b>My Blog</b></p>]\n"
     ]
    }
   ],
   "source": [
    "#attrs参数\n",
    "print(soup.find_all(attrs={'class':'title'}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#recursive参数\n",
    "#调用tag的 find_all() 方法时,Beautiful Soup会检索当前tag的所有子孙节点,\n",
    "#如果只想搜索tag的直接子节点,可以使用参数 recursive=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Python3网络爬虫(三)：urllib.error异常']\n"
     ]
    }
   ],
   "source": [
    "#text参数\n",
    "#通过 text 参数可以搜搜文档中的字符串内容，与 name 参数的可选值一样, \n",
    "#text 参数接受字符串 , 正则表达式 , 列表, True\n",
    "print(soup.find_all(text='Python3网络爬虫(三)：urllib.error异常'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<a class=\"sister\" href=\"http://blog.csdn.net/c406495762/article/details/58716886\" id=\"link1\">Python3网络爬虫(一)：利用urllib进行简单的网页抓取</a>, <a class=\"sister\" href=\"http://blog.csdn.net/c406495762/article/details/59095864\" id=\"link2\">Python3网络爬虫(二)：利用urllib.urlopen发送数据</a>]\n"
     ]
    }
   ],
   "source": [
    "#limit参数 搜索数量的限制\n",
    "#find_all() 方法返回全部的搜索结构,如果文档树很大那么搜索会很慢.\n",
    "#如果我们不需要全部结果,可以使用 limit 参数限制返回结果的数量.\n",
    "#效果与SQL中的limit关键字类似,当搜索到的结果数量达到 limit 的限制时,\n",
    "#就停止搜索返回结果。\n",
    "print(soup.find_all('a',limit=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<p class=\"title\" name=\"blog\"><b>My Blog</b></p>]\n"
     ]
    }
   ],
   "source": [
    "#kwargs参数  这个还是不懂啥意思\n",
    "#如果传入 class 参数,Beautiful Soup 会搜索每个 class 属性为 title 的 tag 。\n",
    "#kwargs 接收字符串，正则表达式\n",
    "print(soup.find_all(class_='title'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
